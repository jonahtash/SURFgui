import os
from multiprocessing import Pool as PoolMP
from multiprocessing.dummy import Pool
from multiprocessing import freeze_support
from datetime import datetime
import subprocess
import csv
import urllib.request
"""from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter
from pdfminer.converter import TextConverter
from pdfminer.layout import LAParams
from pdfminer.pdfpage import PDFPage"""
from io import StringIO
import unicodedata
import ctypes
import requests
import sqlite3
import json
import re
from http.cookiejar import CookieJar
from bs4 import BeautifulSoup
import pandas as pd
import shutil
import xml.etree.ElementTree as ET
import time


"""BEGIN SCIENCE-PARSE SERVER FUNCTIONS"""
"""*****************************"""

#connects to the science-parse server
#server is hosted locally on the machine
#science-parse creates JSON files which contain the PDF contents seperated by section
def _post_science_parse(s):
    a=s.split("+")
    print(a[0])
    try:
        with open(a[0], 'rb') as f:
            r = requests.post('http://localhost:8080/v1', files={a[0]: f})
            open(a[1]+a[0].split('/')[-1][0:-4]+'.json','w',encoding='utf-8').write(r.text)
    #prints an error if the PDF cannot be parsed
    except Exception as e:
        print("ERROR "+str(e))
#gets JSON file from science parse server and saves it to directory
def get_pdf_json(pdf_dir,out_dir,num_thread=2):
    pdf_dir = _clean_path(pdf_dir)
    out_dir = _clean_path(out_dir)
    pool = Pool(num_thread)
    pack = []
    for line in os.listdir(pdf_dir):
        pack.append(pdf_dir+line+"+"+out_dir)
    results = pool.map(_post_science_parse,pack)

#multiprocessing version of "get_pdf_json"
#default number of threads = 2
def get_pdf_json_mp(pdf_dir,out_dir,num_thread=2):
    pdf_dir = _clean_path(pdf_dir)
    out_dir = _clean_path(out_dir)
    pool = PoolMP(num_thread)
    pack = []
    try:
        for line in os.listdir(pdf_dir):
            pack.append(pdf_dir+line+"+"+out_dir)
    except Exception as e:
        print(str(e))
    results = pool.map(_post_science_parse,pack)
    pool.close()

"""***************************"""
"""END SCIENCE-PARSE SERVER FUNCTIONS"""


"""BEGIN JSON PARSING FUNCTIONS"""
"""****************************"""

# takes folder of jsons generated by ScienceParse
# puts jsons into csv formatted file for bkup
# makes specfied modifcation to the json data and outputs to data csv
def run_json_folder(json_path,exclude_path,char_path,bkup_csv_path,csv_out_path):
	json_path = _clean_path(json_path)

	#init sqlite db in memory
	#sql db structure:
	##Table: temp_table
	###Columns: sec_head, text, id, sec_num, inferred, split_num upper_to_lower, digit_to_char, special_to_char
	conn = sqlite3.connect(':memory:')
	cur = conn.cursor()
	cur.execute('CREATE TABLE temp_table (sec_head varchar(255), text TEXT, id INT, sec_num INT, split_num INT, inferred BIT, upper_to_lower FLOAT, digit_to_char FLOAT, special_to_char FLOAT);')

	#go through json files and add their data to table
	for file_path in os.listdir(json_path):
		file_path=json_path+file_path
		print(file_path)
		f = json.load(open(file_path,'r',encoding='utf-8'))
		c=1
		#add title and author entry to table
		if f['metadata']['title']:
			cur.execute("INSERT INTO temp_table VALUES (?, ?, ?, ?, 1, 0, 0, 0, 0)", ('title', _clean_sql(f['metadata']['title']),file_path.split('/')[-1].split('.pdf.json')[0],c))
			c+= 1
		if f['metadata']['authors']:
			cur.execute("INSERT INTO temp_table VALUES (?, ?, ?, ?, 1, 0, 0, 0, 0)", ('authors', _clean_sql(str(f['metadata']['authors'])),file_path.split('/')[-1].split('.pdf.json')[0],c))
			c+= 1

		if f['metadata']['sections']:
			for sec in f['metadata']['sections']:
				text_clean = _clean_sql(sec['text']).replace("N IH -PA Author M anuscript\n",'').replace("N IH -PA Author M anuscript",'')
				heading_clean = ""
				if sec['heading']:
					heading_clean = _clean_sql(sec['heading'])
				else:
					heading_clean = "null"
				cmd = "INSERT INTO temp_table VALUES (?, ?, ?, ?, ?, ?, ?, ? ,? )"
				cur.execute(cmd, (heading_clean, text_clean,file_path.split('/')[-1].split('.pdf.json')[0],c,1,0,_upper_ratio(text_clean),_digit_ratio(text_clean),_special_ratio(text_clean)))
				c+= 1



	#Dump unedited table into backup csv
	pd.read_sql(sql='SELECT * FROM temp_table ORDER BY id,sec_num,split_num', con=conn).to_csv(bkup_csv_path, index=False,sep = ',',quoting=csv.QUOTE_NONNUMERIC)

	#read unwanted headers from csv file
	re = csv.reader(open(exclude_path,'r',encoding='utf-8'))
	for row in re:
		#delete where section header like word in csv
		cur.execute("DELETE from temp_table WHERE sec_head like '%"+row[0]+"%';")
	#remove all sections where the section text is less than certain length-- currently: 600 chars
	#added catch for authors and title since they are most likely too short but still should be included
	cur.execute("DELETE from temp_table WHERE length(text) < 600 AND sec_head!='title' AND sec_head !='authors';")

	#defragment indexing
	cur.execute("SELECT * FROM temp_table ORDER BY id,sec_num,split_num")
	rows = cur.fetchall()
	cur_id = rows[0][2]
	c =1
	for row in rows:
		if not row[2] == cur_id:
			cur_id = row[2]
			c = 1
		cur.execute("UPDATE temp_table SET sec_num = ? WHERE sec_head = ? AND id = ?", (c, row[0], cur_id))
		c+=1

	#go through and split up entries where the section text is longer than split_on characters
	#select all entries with sec. text longer than split_on characters
	split_on = "3000"
	cur.execute('SELECT * FROM temp_table WHERE length(text) > '+split_on+' ORDER BY id,sec_num,split_num;')

	for row in cur.fetchall():
		#split the sec. text of entry into split_on character chunks and interate
		bs = ""
		c_split = 1
		for s in [e+". " for e in row[1].split(". ") if e]:
			#insert chunk of text into table
			if len(bs + s) > int(split_on):
				cur.execute("INSERT INTO temp_table VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)",(row[0],bs,str(row[2]),row[3],c_split,row[5],_upper_ratio(bs),_digit_ratio(bs),_special_ratio(bs)))
				c_split+= 1
				bs = s
			else:
				bs+=s
		#remove old entry
		cur.execute("DELETE from temp_table WHERE sec_head like ? AND id=? AND length(text)>"+split_on+";",(row[0],str(row[2])))

	##try to fix null headers
	#get all rows with sec_head as null
	cur.execute("SELECT * FROM temp_table WHERE sec_head like 'null'")
	rows = cur.fetchall()
	#set variables with wider scope that names can persist
	cur_head = 'null'
	cur_id = rows[0][2]
	inf = 0
	for row in rows:
		#if new doc reset
		if cur_id != row[2]:
			cur_id = row[2]
			cur_head = 'null'
			inf=0
		text = row[1].strip()
		#if the first word is all caps it's probably a title
		#check if all caps

		if (' ' in text and text[:text.index(' ')].upper().strip() == text[:text.index(' ')].strip() and text[:text.index(' ')].isalpha()):
			a=text.split(' ')
			c=0
			bs = ""
			#read words until words are no longer all caps
			while(c < len(a) and a[c]==a[c].upper()):
				bs += a[c]+" "
				c+=1
			#set to be header
			if len(bs[:-1])>1:
                            cur_head = bs[:-1]
                            inf = 1
		else:
			#open csv with characters that denote heading
			#for each character see if that character comes before the end of the first sentence
			for line in csv.reader(open(char_path, 'r')):
				try:
					i = text.index(line[0].strip())
					#check to see if text in this format
					#Subheading in line: It is important that the character after the colon(or any char in csv) is captial. More text...
					if(i<text.index('.') and text[i+1:i+3].strip().isupper()):
						cur_head = text[:i]
						inf = 1
				except:
					#idk python
					s='||-//'
		#update entry in table with new sec_head and if that sec_head was inferred (i.e changed by this code block)
		cur.execute("UPDATE temp_table SET sec_head = ?, inferred = ? WHERE sec_num = ? AND id = ? AND split_num = ?", (cur_head, inf, row[3], row[2], row[4]))

	#delete entries based on text statistics
	#the both constants are for when entries have both text stats greater than a given value
	both_upper = .1
	both_digit = .1
	#for when just upper_to_lower is greater than certain ratio
	uppper = .15
	#for when just digit_to_char is greater than certain ratio
	digit = .12
	cur.execute("DELETE from temp_table WHERE (upper_to_lower > ? AND digit_to_char > ?) OR upper_to_lower > ? OR digit_to_char > ?;",(both_upper,both_digit,uppper,digit))

	#get the remaining entries in the table and write them to csv
	pd.read_sql(sql='SELECT * FROM temp_table ORDER BY id,sec_num,split_num', con=conn).to_csv(csv_out_path, index=False,sep = ',',quoting=csv.QUOTE_NONNUMERIC)


def partition_jsons(json_dir,partition_dir,json_per_folder):
    files = [os.path.join(json_dir, f) for f in os.listdir(json_dir)]
    i = 0
    curr_subdir = None
    name = ""
    if len(json_dir.split("/")[-1])>1:
        name = json_dir.split("/")[-1]
    else:
        name=json_dir.split("/")[-2]

    for f in files:
        # create new subdir if necessary
        if i % json_per_folder == 0:
            subdir_name = os.path.join(partition_dir, name+'{0:04d}'.format(int(i / json_per_folder + 1)))
            if not os.path.exists(subdir_name):
                os.mkdir(subdir_name)
            curr_subdir = subdir_name

        # move file to current dir
        f_base = os.path.basename(f)
        shutil.copy(f, os.path.join(subdir_name, f_base))
        i += 1

def run_partition_folders(part_folder_dir,out_csv_dir,exclude_path,char_path,limit=-1):
    c = 0
    out_csv_dir = _clean_path(out_csv_dir)
    for f in os.listdir(part_folder_dir):
        folder = os.path.join(part_folder_dir, f)
        print(folder)
        run_json_folder(folder,exclude_path,char_path,out_csv_dir+f+"bkup.csv",out_csv_dir+f+"data.csv")
        if limit>0 and c>=limit:
            break

"""**************************"""
"""END JSON PARSING FUNCTIONS"""




"""BEGIN UTILITY FUNCTIONS"""
"""***********************"""

#Removes duplicate lines in txt file located at in_path
#Outputs to out_path
def _remove_dupes_txt(in_path, out_path):
    lines_seen = set()
    outfile = open(out_path, "w")
    for line in open(in_path, "r"):
        if line not in lines_seen:
            outfile.write(line)
            lines_seen.add(line)
    outfile.close()

#Counts the number of txts in id_txt_list.
#Counts the number of pdfs in pdf_dir_list.
#Returns the sum of pdf and txts: sum should equal 96961.
def get_count(id_txt_list, pdf_dir_list):
    c=0
    for txt in id_txt_list:
        c+= sum(1 for line in open(txt))
    for d in pdf_dir_list:
        c+=len(os.listdir(d))
    return c

def _any_rev(array,string):
    for i in array:
        if i in string:
            return True
    return False
#uses PDFResource Manager tool to convert the PDF files to text
#not used
def _convert_pdf_to_txt(path):
    rsrcmgr = PDFResourceManager()
    retstr = StringIO()
    codec = 'utf-8'
    laparams = LAParams()
    device = TextConverter(rsrcmgr, retstr, codec=codec, laparams=laparams)
    fp = open(path, 'rb')
    interpreter = PDFPageInterpreter(rsrcmgr, device)
    password = ""
    maxpages = 0
    caching = True
    pagenos=set()

    for page in PDFPage.get_pages(fp, pagenos, maxpages=maxpages, password=password,caching=caching, check_extractable=True):
        interpreter.process_page(page)

    text = retstr.getvalue()

    fp.close()
    device.close()
    retstr.close()
    return text

def rem_pmcid(in_path,out_path):
    with open(in_path,'r') as inF:
        with open(out_path,'w') as outF:
            for line in inF:
                outF.write(line.split('+')[1])

def _split_every(n, s):
    return [ s[i:i+n] for i in range(0, len(s), n) ]

def _clean_path(path):
    if path[-1]!="/":
        return path+"/"
    else:
        return path

def _clean_sql(s):
    return ''.join([i if ord(i) < 128 else '' for i in s]).replace("\t",' ').replace("\n",' ').replace("\r"," ")

def get_empty_files(pdf_dir,out_path):
    out = open(out_path,'w')
    for f in os.listdir(os.fsencode(pdf_dir)):
        if os.stat(str(pdf_dir)+"/"+f.decode('utf-8')).st_size == 0:
            out.write(f.decode('utf-8').split("/")[-1].split(".pdf")[0]+"\n")
    out.close()
#sorts out the flies which do not have a DOI or PMCID
#these files are unretrievable
def sort_nonretrievable(csv_file_path, good_out_path, bad_out_path):
    good_pdf= open(good_out_path, 'w')
    bad_pdf = open(bad_out_path, 'w')
    with open(csv_file_path, encoding='utf-8') as csvf:
        readCSV = csv.reader(csvf, delimiter=',')
        for row in readCSV:
            if "doi:" not in row[3] and "PMCID" not in row[7]:
                print("BAD: "+row[9])
                bad_pdf.write(row[9]+"\n")
            else:
                print("GOOD: "+row[9])
                good_pdf.write(row[9]+"\n")

        good_pdf.close()
        bad_pdf.close()

def _txt_diff(txt1,txt2,out_txt):
    t1 = open(txt1,'r').readlines()
    t2 = open(txt2,'r').readlines()
    open(out_txt,'w').writelines(list(set(t1)-set(t2)))

#computes the ratio of uppercase letters to all characters
#used to sort the parsed JSON sections
def _upper_ratio(s):
    if len(s)==0:
        return 0
    u = len(re.findall('[A-Z]',s))
    l = len(re.findall('[a-z]',s))
    if u+l==0:
        return 0
    return u/(u+l)
#computes the ratio of numerical characters to characters in a section of parsed PDF
#used to sort sections that are not needed
def _digit_ratio(s):
    if len(s)==0:
        return 0
    d = len(re.findall('[0-9]',s))
    c = len(re.findall('[A-z]',s))
    if c+d==0:
        return 0
    return d/(c+d)


#computes the ratio of special characters to characters in a section of parsed PDF
def _special_ratio(s):
    if len(s)==0:
        return 0
    c = len(re.findall('[A-z]',s))
    s = len(re.findall('[\\.\\!\\@\\#\\$\\%\\^\\&\\*\\(\\)\\_\\+\\-\\=]',s))
    if c+s==0:
        return 0
    return s/(c+s)

"""*********************"""
"""END UTILITY FUNCTIONS"""

def run_csv_output(pdf_in, csv_out):
    json_path = (pdf_in if pdf_in[-1] == '/' else pdf_in+'/') + 'json'
    if not os.path.exists(json_path):
        os.makedirs(json_path)
    print("Generating section JSONs...")    
    os.system("java -Xmx10g -jar sp.jar "+pdf_in+" -o "+json_path)
    print("Generating CSV tables ..")
    run_json_folder(json_path,'D:\\Jonah\\Documents\\NIST\\Repos\\SURFgui\\wrappers\\python\\exclude.csv','D:\\Jonah\\Documents\\NIST\\Repos\\SURFgui\\wrappers\\python\\chars.csv',csv_out+"\\backup_files",csv_out)
